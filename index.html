<!DOCTYPE html>
<html lang="en">
<head>

	<!-- Basic Page Needs
  ================================================== -->
	<meta charset="utf-8">
	<title>JuliaDiff</title>
	<meta name="description" content="Home of the JuliaDiff project.">
	<meta name="author" content="Miles Lubin">

	<!-- Mobile Specific Metas
  ================================================== -->
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<!-- CSS
  ================================================== -->
	<link rel="stylesheet" href="stylesheets/base.css">
	<link rel="stylesheet" href="stylesheets/skeleton.css">
	<link rel="stylesheet" href="stylesheets/layout.css">

	<!-- Favicons
	================================================== -->
    <!-- <link rel="shortcut icon" href="images/favicon.ico"> -->
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-44252521-6', 'juliadiff.org');
  ga('send', 'pageview');

</script>
</head>
<body>



	<!-- Primary Page Layout
	================================================== -->

	<div class="container">

<!-- Header -->
    <div class="sixteen columns">
      <h1 class="remove-bottom" style="margin-top: 40px; color: #ad3f39;">JuliaDiff</h1>
      <h5>
        Differentiation tools in <a href="http://julialang.org">Julia</a>.
        <a href="http://github.com/JuliaDiff/">JuliaDiff on GitHub.</a>
      </h5>
      <hr />
    </div>


    <div id="home">
        <h2>Stop approximating derivatives!</h2>

        <p> Derivatives are required at the core of many numerical algorithms. Unfortunately, they are usually computed <em>inefficiently</em> and <em>approximately</em> by some variant of the finite difference approach
        $$ f'(x) \approx \frac{f(x+h) - f(x)}{h}, h \text{ small }. $$
        This method is <em>inefficient</em> because it requires \( \Omega(n) \) evaluations of \( f : \mathbb{R}^n \to \mathbb{R} \) to compute the gradient \( \nabla f(x) = \left( \frac{\partial f}{\partial x_1}(x), \cdots, \frac{\partial f}{\partial x_n}(x)\right) \), for example. It is <em>approximate</em> because we have to choose some finite, small value of the step length \( h \), balancing floating-point precision with mathematical approximation error.
        </p>

        <h4>What can we do instead?</h4>
        <p>One option is to explicitly write down a function which computes the exact derivatives by using the rules that we know from Calculus. However, this quickly becomes an error-prone and tedious exercise. <strong>There is another way!</strong> The field of <a href="http://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a> provides methods for automatically computing <em>exact</em> derivatives (up to floating-point error) given only the function \( f \) itself. Some methods use many fewer evaluations of \( f \) than would be required when using finite differences. In the best case, <strong>the exact gradient of \( f \) can be evaluated for the cost of \( O(1) \) evaluations of \( f \) itself</strong>.  The caveat is that \( f \) cannot be considered a black box; instead, we require either access to the source code of \( f \) or a way to plug in a special type of number using operator overloading.  </p>

        <h2> What is JuliaDiff? </h2>
        <p> JuliaDiff is an informal organization which aims to unify and document packages written in <a href="http://julialang.org">Julia</a> for evaluating derivatives. The technical features of Julia, namely, multiple dispatch, source code via reflection, JIT compilation, and first-class access to expression parsing make implementing and using techniques from automatic differentiation easier than ever before (in our biased opinion). Packages hosted under the JuliaDiff organization follow the same guidelines as for <a href="http://juliaopt.org/">JuliaOpt</a>; namely, they should be actively maintained, well documented and have a basic testing suite.</p>

        <h3>Included packages</h3>
        <p>Below we list the packages that are currently included in the JuliaDiff organization and their testing status on the latest Julia release, if available.</p>

	<table style="display: block; margin-left: 5%; margin-right: 5%; width: 90%;">
		<tr>
		  <td style="padding: 10px 10px">
			<a href="https://github.com/JuliaDiff/ForwardDiff.jl"><strong>ForwardDiff</strong></a><br>
			<a href="http://pkg.julialang.org/?pkg=ForwardDiff&ver=0.6">
			  <img src="http://pkg.julialang.org/badges/ForwardDiff_0.6.svg" alt="Release testing status" style="margin-top: 2px;">
			</a>
		  </td>
		  <td>
			ForwardDiff provides user-friendly methods for performing forward-mode automatic differentiation on native Julia code. ForwardDiff offers several advanced features, such as nested differentiation, a non-allocating API, and a SIMD-vectorizable multidimensional dual number type.
		  </td>
		</tr>

		<tr>
		  <td style="padding: 10px 10px">
			<a href="https://github.com/JuliaDiff/ReverseDiff.jl"><strong>ReverseDiff</strong></a><br>
			<a href="http://pkg.julialang.org/?pkg=ReverseDiff&ver=0.6">
			  <img src="http://pkg.julialang.org/badges/ReverseDiff_0.6.svg" alt="Release testing status" style="margin-top: 2px;">
			</a>
		  </td>
		  <td>
			ReverseDiff provides user-friendly methods for performing reverse-mode automatic differentiation on native Julia code. ReverseDiff implements many linear algebra optimizations, flexible performance annotations, and composes well with ForwardDiff (enabling mixed-mode AD).
		  </td>
		</tr>

		<tr>
		  <td style="padding: 10px 10px">
			<a href="https://github.com/lbenet/TaylorSeries.jl"><strong>TaylorSeries</strong></a><br>
			<a href="http://pkg.julialang.org/?pkg=TaylorSeries&ver=0.6">
			  <img src="http://pkg.julialang.org/badges/TaylorSeries_0.6.svg" alt="Release testing status" style="margin-top: 2px;">
			</a>
		  </td>
		  <td>
			TaylorSeries implements truncated multivariate power series for high-order integration of ODEs and forward-mode automatic differentiation of arbitrary order derivatives via operator overloading.
		  </td>
		</tr>

          <tr>
            <td style="padding: 10px 10px">
              <a href="https://github.com/JuliaDiff/DualNumbers.jl"><strong>DualNumbers</strong></a><br>
              <a href="http://pkg.julialang.org/?pkg=DualNumbers&ver=0.6">
                <img src="http://pkg.julialang.org/badges/DualNumbers_0.6.svg" alt="Release testing status" style="margin-top: 2px;">
              </a>
            </td>
            <td>
             DualNumbers provides a dual number type which can be used to take first-order scalar derivatives of native Julia code.
            </td>
          </tr>

          <tr>
            <td style="padding: 10px 10px">
              <a href="https://github.com/JuliaDiff/HyperDualNumbers.jl"><strong>HyperDualNumbers</strong></a><br>
              <a href="http://pkg.julialang.org/?pkg=HyperDualNumbers&ver=0.6">
                <img src="http://pkg.julialang.org/badges/HyperDualNumbers_0.6.svg" alt="Release testing status" style="margin-top: 2px;">
              </a>
            </td>
            <td>
             HyperDualNumbers provides a hyper-dual number type which can be used to take first-order and second-order scalar derivatives of native Julia code.
            </td>
          </tr>
	  </table>

        <h3>Related packages</h3>

        <p>These Julia packages also provide differentiation functionalities.</p>

        <table style="display: block; margin-left: 5%; margin-right: 5%; width: 90%;">
          <tr>
            <td style="padding: 10px 10px">
              <a href="https://github.com/gaika/AutoDiffSource.jl"><strong>AutoDiffSource</strong></a><br>
              <a href="http://pkg.julialang.org/?pkg=AutoDiffSource&ver=0.6">
                <img src="http://pkg.julialang.org/badges/AutoDiffSource_0.6.svg" alt="Release testing status" style="margin-top: 2px;">
              </a>
            </td>
            <td>
		Fast reverse mode differentiation for scalar and tensor functions using source code transformation.
            </td>
          </tr>

          <tr>
            <td style="padding: 10px 10px">
              <a href="https://github.com/johnmyleswhite/Calculus.jl"><strong>Calculus</strong></a><br>
              <a href="http://pkg.julialang.org/?pkg=Calculus&ver=0.6">
                <img src="http://pkg.julialang.org/badges/Calculus_0.6.svg" alt="Release testing status" style="margin-top: 2px;">
              </a>
            </td>
            <td>
              Provides methods for symbolic differentiation and finite-difference approximations.
            </td>
          </tr>

	  <tr>
            <td style="padding: 10px 10px">
              <a href="https://github.com/JuliaOpt/JuMP.jl"><strong>JuMP</strong></a><br>
              <a href="http://pkg.julialang.org/?pkg=JuMP&ver=0.6">
                <img src="http://pkg.julialang.org/badges/JuMP_0.6.svg" alt="Release testing status" style="margin-top: 2px;">
              </a>
            </td>
            <td>
              An algebraic modeling language for optimization with an internal implementation of reverse-mode automatic differentiation for gradients and sparse Hessian matrices given closed-form expressions.
	      Includes support for automatic differentiation of user-provided functions.
            </td>
          </tr>

          <tr>
            <td style="padding: 10px 10px">
              <a href="https://github.com/jwmerrill/PowerSeries.jl"><strong>PowerSeries</strong></a><br>
              <a href="http://pkg.julialang.org/?pkg=PowerSeries&ver=0.6">
                <img src="http://pkg.julialang.org/badges/PowerSeries_0.6.svg" alt="Release testing status" style="margin-top: 2px;">
              </a>
            </td>
            <td>
              Implements truncated power series type which can be used for forward-mode automatic differentiation of arbitrary order derivatives via operator overloading.
            </td>
          </tr>

          <tr>
            <td style="padding: 10px 10px">
              <a href="https://github.com/symengine/SymEngine.jl"><strong>SymEngine</strong></a><br>
              <a href="http://pkg.julialang.org/?pkg=SymEngine&ver=0.6">
                <img src="http://pkg.julialang.org/badges/SymEngine_0.6.svg" alt="Release testing status" style="margin-top: 2px;">
              </a>
            </td>
            <td>
              Implements symbolic differentiation.
            </td>
          </tr>
        </table>

    <h3>How are they being used?</h3>
    <p>Packages implementing automatic differentiation techniques are already in use in the broader Julia ecosystem:</p>

    <ul>
        <li><a href="https://github.com/JuliaOpt/Optim.jl">Optim</a> and <a href="https://github.com/EconForge/NLsolve.jl">NLsolve</a> use <strong>ForwardDiff</strong> to compute exact gradients and Jacobians of user-provided functions. Refer to each package's documentation to enable AD.</li>
        <li><a href="https://github.com/JuliaStats/Klara.jl">Klara</a>, an MCMC engine, uses <strong>ForwardDiff</strong> and <strong>ReverseDiff</strong> to compute gradients and Hessians of (possibly unnormalized) target distributions.</li>
    </ul>

    <h2>Links</h2>
    <p>Discussions on JuliaDiff and its uses may be directed to the <a href="https://discourse.julialang.org/">Julia Discourse forum</a> or the <a href="https://groups.google.com/forum/#!forum/julia-opt">julia-opt</a> mailing list. The site <a href="http://www.autodiff.org/">autodiff.org</a> serves as a portal for the academic community. For a well-written simple introduction to reverse-mode automatic differentiation, see Justin Domke's <a href="http://justindomke.wordpress.com/2009/03/24/a-simple-explanation-of-reverse-mode-automatic-differentiation/">blog post</a>. Finally, automatic differentiation techniques have been implemented in a variety of languages. If you would prefer not to use Julia, see the <a href="http://en.wikipedia.org/wiki/Automatic_differentiation">wikipedia page</a> for a comprehensive list of available packages.
</div>

<div class="sixteen columns">
      <hr />
      <small>This website was made with <a href="http://www.getskeleton.com">Skeleton</a>, based on Iain Dunning's work on <a href="http://juliaopt.org">JuliaOpt</a>. Something wrong? Submit <a href="https://github.com/JuliaDiff/juliadiff.github.io/issues">issue</a>.
    </div>
	</div><!-- container -->


<!-- End Document
================================================== -->
</body>
</html>
